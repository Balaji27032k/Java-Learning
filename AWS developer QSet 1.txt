You've created a Lambda function that requires scheduled execution. What are the best methods to meet this requirement effectively?
A)	
Use Cloudtrail to schedule the Lambda function.
B)	
Use the schedule service in AWS Lambda.
C)	
Use an EC2 Instance to schedule the Lambda invocation.
D)	
Use Cloudwatch events to schedule the Lambda function.

The engineers at EpicONE Games are determining the cache memory and node provisioning size for their AWS ElastiCache cluster to support high throughput, low latency, and approximately 35 to 45 million users per session for their upcoming war game. They have been advised to distribute the memory over more nodes with smaller capacities rather than using fewer nodes with larger capacities. With a requirement of 35 GB of cache memory for the cluster, one of the engineers has proposed several options. How would you assist the team in choosing the correct sizing from the proposed options?
A)	
The number of nodes in the cluster is not a key factor when it comes to cluster availability.
B)	
Use cache.t2.medium type of nodes with each having 3.22 GiB memory, 2 cores, and a quantity of 11 nodes giving 35.42 GiB in total meeting the requirement.
C)	
Start with one cache.m5.large node. Monitor memory usage, CPU utilization, and cache hit rate with ElastiCache metrics that further get published to CloudWatch. These metrics will enable monitoring of your clusters in the form CPU usage and cache gets and cache misses. These metrics will get measured and published for each Cache node in 60-second intervals. Based on the results, node and memory sizing can then be worked upon.
D)	
The node size does not impact the performance and fault tolerance at all. You could start with the t2 type of instances keeping the cost to a minimum and then add the nodes as the requirement increases.

You have a launch template where a subnet is specified in its network interface. Now you need to use AWS CLI (aws autoscaling create-auto-scaling-group) to create an Auto Scaling group with the launch template. However, the ASG should be launched in another subnet that is different from the one specified in the launch template. How would you create the ASG with the AWS CLI command?
A)	
Create another launch template with the new subnet in its network interface. In AWS CLI (aws autoscaling create-auto-scaling-group), specify the same subnet through the --vpc-zone-identifier option.
B)	
Modify the subnet in the launch template and create the ASG using the latest version of the launch template.
C)	
Change the subnet in the launch template. In AWS CLI (aws autoscaling create-auto-scaling-group), specify the subnet through the --launch-template option.
D)	
Specify the subnet through the --vpc-zone-identifier option in AWS CLI (aws autoscaling create-auto-scaling-group). The subnet in the launch template is ignored.

You have been assigned to manage the database administration for a web application where the database was initially created by a former DB admin. The manager has requested your assistance in implementing a new reporting feature that requires querying a DynamoDB table using attributes that are not designated as Partition Key and Sort Key. How would you recommend resolving this issue?
A)	
Edit the table and change the existing Partition and Sort Key to the required ones.
B)	
Clone the existing table and change the Partition and Sort Key during the table creation process.
C)	
Create a Local Secondary Index with required attributes as Partition and Sort Key.
D)	
Create Global Secondary Index and set required attributes as Partition and Sort Key.

When working with a new HTTP based application on Amazon EC2 and DynamoDB, the AWS API is being utilized to retrieve service graph data. The Team Lead has recommended using the GetTraceSummaries API to obtain trace summaries. What flags can be used when retrieving traces with the GetTraceSummaries API? Choose two answers from the options listed below.
A)	
Event Time
B)	
UserId
C)	
Annotations
D)	
HTTPMethod
E)	
TraceID

Consider deploying a mobile application for an eCommerce company that necessitates theme changes based on the device type. Android devices should display a green theme, while iOS devices should display a blue theme. It is recommended to utilize AWS Lambda for the mobile backend deployment. What is the most efficient way to achieve this?
A)	
Use the "event" object to determine which device invoked the function.
B)	
Use a REST client to pass device info to API Gateway that invokes AWS Lambda function.
C)	
Different environment variables for Android and iOS.
D)	
Use AWS Mobile SDK and use the "context" object.

An engineering company seeks to acquire storage space through an Amazon S3 bucket to store its project documents. The data will be kept for an extended period, although the company is uncertain about the access pattern for this information. Which storage class would offer a cost-efficient solution without affecting performance?
A)	
S3 One Zone-IA
B)	
S3 Intelligent-Tiering
C)	
S3 Standard-IA
D)	
S3 Standard

You are tasked with creating a serverless web application utilizing DynamoDB as the database. The client's requirement is to track all data changes in DynamoDB and save them as separate files on S3. How would you go about accomplishing this?
A)	
Use DynamoDB binary logging feature to record each and every change on S3.
B)	
Write a script that will record each and every create or update request and store it on S3.
C)	
Enable DynamoDB streams and attach them to Lambda which will read the data and store it on S3.
D)	
Enable DynamoDB streams and set the destination to S3. This will automatically store each change on S3.

As a developer, you need to access resources in a different account. What is the most effective method to accomplish this task?
A)	
Create an IAM Role and attach it to an EC2 Instance.
B)	
Create a cross-account role and call the AssumeRole API.
C)	
Create a user in the destination account and share the password.
D)	
Create a user in the destination account and share the Access Keys.

As a developer responsible for a containerized application, you have been instructed to configure dynamic port mapping for Amazon ECS and implement load balancing. What is the accurate statement regarding this scenario?
A)	
If dynamic port mapping is set up correctly, then you see the registered targets and the assigned port for the task.
B)	
Classic Load Balancer allows you to run multiple copies of a task on the same instance.
C)	
Application Load Balancer uses static port mapping on a container instance.
D)	
After creating an Amazon ECS service, you add the load balancer configuration.

You have been brought on board by a company to work on their continuous development project. This project involves streaming data from different log sources onto Amazon Kinesis streams. Your task is to analyze real-time data using standard SQL. What options are available for achieving this goal?
A)	
Amazon EMR
B)	
Amazon Kinesis Firehose
C)	
Amazon Kinesis Data Analytics
D)	
Amazon Athena

After creating a Lambda function and attempting to debug it post-execution, you insert print statements into the code to aid in the debugging process. However, upon checking Cloudwatch logs, you discover that there are no logs for the Lambda function. What might be the root cause of this issue?
A)	
There is not enough time assigned to the function.
B)	
You've not enabled versioning for the Lambda function.
C)	
The IAM role needed for the lambda function to write the logs to Cloudwatch logs does not have the necessary permissions.
D)	
There is not enough memory assigned to the function.

You are working on a REST API and need to pass client-submitted method requests directly to a Lambda function. Which integration type should you set for this requirement?
A)	
"type": "mock"
B)	
"type": "aws_proxy"
C)	
"type": "aws"
D)	
"type": "http_proxy"

Your team is in the process of developing an application, and it is necessary to implement an Amazon Elastic File System (EFS) to facilitate data sharing among various nodes. In order to ensure security within the EFS file system, it is essential to establish a policy that enforces specific default security rules. For instance, disabling root access and mandating the use of TLS for connections from EFS clients are among the requirements. What would be the most appropriate approach to achieve this?
A)	
Add the TLS option when mounting the file system with the EFS mount helper.
B)	
Attach an IAM identity policy to each IAM entity to perform specific actions on the file system.
C)	
Configure an EFS file system policy to control NFS client access to the EFS resource.
D)	
Create an EFS service-linked role and attach it to the file system.

Throughout the process of developing, defining, and deploying a backend for your project, it is essential to carefully consider the utilization of user and identity pools within the context of a serverless application. Which assertions hold true in this particular situation?
A)	
User pools are for authorization (access control) which is fully managed on your behalf.
B)	
User pools support temporary, limited-privilege AWS credentials to access other AWS services while Identity pools provide sign-up and sign-in services.
C)	
User pools help you track user device, location, and IP address while Identity pools help you generate temporary AWS credentials for unauthenticated users.
D)	
User pools give your users access to AWS resources.
---------------------------------------------------------------------------------------------------------------------------------------
When creating an application, it is advisable to incorporate MFA and password recovery as extra measures to enhance security through the implementation of dual authentication and recovery processes. What is the best approach to take in this situation?
A)	
Use SMS as a second factor and TOTP along with a security key as the MFA device for your IAM and root users.
B)	
Use TOTP as a second factor and SMS as a password recovery mechanism which is disjoint from an authentication factor.
C)	
Enable MFA as Required immediately after creating a user pool to add another layer of security.
D)	
Disable adaptive authentication, so you can configure a second factor authentication in response to an increased risk level.

Which of the next options could you use as MFA in Cognito? Choose two answers from the options listed below.
A)	
SES
B)	
Google Authenticator
C)	
Cognito Identity Pool
D)	
SMS text message with MFA code

As an API developer for a major manufacturing company, you have created an API resource that enables the addition of new products to the distributor's inventory through a POST HTTP request. This resource includes an Origin header and accepts application/x-www-form-encoded as the request content type. What response header should be used to grant access to this resource from a different origin?
A)	
All of the below
B)	
Access-Control-Allow-Origin
C)	
Access-Control-Request-Method
D)	
Access-Control-Request-Headers

You have created a series of scripts utilizing AWS Lambda. These scripts require access to EC2 Instances within a VPC. What steps should be taken to guarantee that the AWS Lambda function can reach the resources in the VPC? Choose two answers from the options listed below.
A)	
Ensure that the VPC Flow Log IDs are configured in the Lambda function.
B)	
Ensure that the subnet IDs are configured in the Lambda function.
C)	
Ensure that the NACL IDs are configured in the Lambda function.
D)	
Ensure that the Security Group IDs are configured in the Lambda function.

You are creating a Lambda function in your Dev1 account with the goal of saving QR codes in two S3 buckets. One bucket (BucketDev1) is in the same account as the Lambda, while the second bucket (BucketDev2) is in another AWS account called Dev2. The requirement is to restrict access to only allow this specific Lambda from Dev1 to create objects in BucketDev2. What permissions are needed to grant access to this Lambda function for both S3 buckets?
A)	
Create two policies, the first one allows Lambda to create objects in BucketDev1 and the second, allows Lambda to create objects in BucketDev2. Create an IAM role with these two policies and attach it to the Lambda.
B)	
Create a role called S3-Lambda with a policy that allows to create objects in any S3 bucket and attach it to the Lambda, create in BucketDev1 a bucket policy that only allows resources with the S3-Lambda role to write in BucketDev1.
C)	
Create a role called S3-Lambda with a policy that allows to create objects in any S3 bucket and attach it to the Lambda, create in BucketDev2 a bucket policy that allows resources with the S3-Lambda role from Dev1 account to write in BucketDev2.
D)	
Create a role called S3-Lambda with a policy that allows you to write in BucketDev1 and BucketDev2 and attach it to the Lambda. Create in BucketDev2 a bucket policy with two statements. The first statement denies all resources with the S3-Lambda role of Dev1 account to write in any S3 bucket, the second statement allows the specified Lambda from Dev1 account to write in the specified bucket in dev2 account.

What can be specified to provide the Linux user from the third-party software firm with the minimum access permissions required to execute dependencies before the build phase of the new application deployment for your firm?
A)	
Specify run-as at the top of pre_build command of phases block.
B)	
Specify run-as for each command in the env block.
C)	
Specify run-as at the top of build command of phases block.
D)	
Specify run-as at the top of the buildspec file.

You possess a VPC with a subnet containing an AWS RDS instance. Your supervisor has requested that you begin monitoring all changes made to its data by applications. You opt to utilize a Lambda function for this purpose. However, you must grant the function access to the VPC. How do you plan to accomplish this?
A)	
Create a Role to allow access between Lambda and VPC.
B)	
Create a Role to allow access between Lambda and RDS.
C)	
Create a Policy and a Role to allow access between Lambda and RDS.
D)	
Create a Role to allow access between Lambda and the subnet.

You are developing an application using AWS Lambda, where a Lambda function is placed in a private subnet without internet access. This Lambda function will receive bank account information as a JSON object (about 7 KB in size). To satisfy the requirement of encrypting this information before saving it to DynamoDB, which approach should you use?
A)	
Generate a new Customer Master Key. Encrypt the JSON data with the encrypt() KMS method using the CMS previously generated. Upload the encrypted data to DynamoDB.
B)	
Create a VPC Endpoint in the Lambda VPC for the KMS Service. Generate a new Customer Master Key. Encrypt the JSON data using KMS Envelope Encryption. Upload the encrypted data to DynamoDB.
C)	
Generate a new Customer Master Key. Encrypt the JSON data using KMS Envelope Encryption. Upload the encrypted data to DynamoDB.
D)	
Create a VPC Endpoint in the Lambda VPC for the KMS Service. Generate a new Customer Master Key. Encrypt the JSON data with the encrypt() KMS method using the CMS previously generated. Upload the encrypted data to DynamoDB.

A Junior Engineer is setting up the AWS X-Ray daemon to run locally on a multi-vendor operating system environment. He is worried about the configuration of the listening port for this particular situation. What is the accurate statement regarding the listening port for the AWS X-Ray Daemon?
A)	
AWS X-Ray Daemon Listening Port is default as 2000 and cannot be changed.
B)	
AWS X-Ray Daemon Listening Port can be changed only for the Linux environment.
C)	
AWS X-Ray Daemon Listening Port cannot be changed while running the daemon locally.
D)	
AWS X-Ray Daemon Listening Port can be changed using --bind command with CLI.

As a team lead in your company, you have been tasked with overseeing the Blue Green Deployment methodology for a specific application. What are some of the strategies that can be utilized to effectively implement this methodology? Choose two answers from the options listed below.
A)	
Using Elastic Beanstalk with the swap URL feature
B)	
Using Autoscaling Groups to scale on demands for both deployments
C)	
Using Route 53 with Weighted Routing policies
D)	
Using Route 53 with Latency Routing policies

A well-known supplier of online learning platforms, with more than 300 microservices hosting its 500+ courses, is facing bottlenecks that cause server errors due to timeouts and high CPU consumption during the JSON decentralization process, which is used for application interactions. Manual code reviews are becoming excessively time-consuming and labor-intensive. Could you please choose the appropriate option from the list below to fix the high CPU utilization, performance deterioration, and application communication latencies?
A)	
Upgrade the backend EC2 Instance family to the latest Amazon EC2 M5 type of instances providing 25 Gbps of network bandwidth using enhanced networking.
B)	
Migrate the entire workload to AWS Elastic Kubernetes Services resulting in increased performance, automatic scaling and applications availability.
C)	
Move the code to a Test environment and Integrate Amazon CodeReview Profiler to evaluate the performance of JSON code and native object serialization.
D)	
Change the Application Load balance to Network Load balancer adding more listeners.

You possess a SAM template for deploying a Lambda function, and you are currently in the process of developing a new version. Your supervisor has requested that you promptly redirect traffic once the new version has been built and tested. What is the most effective, seamless, and straightforward method to accomplish this task?
A)	
Set AutoPublishAlias property of the function resource.
B)	
Use Qualifiers in the Management Console to select the version which you built and tested.
C)	
Point an alias, PROD to the version which you built and tested.
D)	
Set DeploymentPreference property of the function resource.

You are currently engaged in a new project that requires the development of Lambda functions for a Serverless Web Application. A substantial team of developers will be collaborating on this project, making various modifications to the Lambda function code. It is recommended that you adhere to best practices by releasing a version when either creating a Lambda function or updating its code. What statement accurately describes the published version of a Lambda Function?
A)	
A published version is a snapshot of function code and configuration that can't be changed.
B)	
A published version is a snapshot of function code and has the same version number when a previous version is deleted and recreated.
C)	
A published version is a snapshot of function code and has a unique Amazon Resource Name that can be updated using "UpdateFunctionCode".
D)	
A published version is a snapshot of function code and configuration that can be updated using the "Publish" parameter.

The Development Team has effectively implemented a new application on the Amazon EC2 instance in the us-west-2 region. They now require your assistance in deploying the same application on Amazon EC2 instances in other regions. Specifically, they need support in deploying EC2 instances with images created from instances in the us-west-2 region, while ensuring that the latest security patches are applied. What would be the most appropriate action to meet this requirement?
A)	
Use EC2 Image Builder to create images with the latest patches.
B)	
Create a snapshot of an image and deploy it in all other EC2 instances.
C)	
Build automated scripts to create updated images with the latest patches.
D)	
Manually create EC2 image with the latest patches.

The development team is currently strategizing the migration of an on-premise data store to AWS DynamoDB. The previous database had defined triggers for updating existing items. What is the simplest method to replicate this functionality in DynamoDB after the migration?
A)	
Define SQS topics in response to events from DynamoDB Streams.
B)	
Define triggers in DynamoDB for each table.
C)	
Define Lambda functions in response to events from DynamoDB Streams.
D)	
Define SNS topics in response to events from DynamoDB Streams.

You work as a developer for a firm that has created a serverless application enabling users to do online payment transactions. The apps comprise multiple Lambda functions and a DynamoDB table. This is accomplished with a SAM template. Nevertheless, individuals are unable to access their transaction history or modify their payment details. Upon troubleshooting, it is shown that the Lambda functions lack the necessary rights to retrieve data from the DynamoDB table. How would you address this problem by implementing a more strict and secure AWS Managed Policy?
A)	
Create an IAM role using AWS Managed Policy AmazonDynamoDBReadOnlyAccess and attach to Lambda functions.
B)	
Create an IAM role using AWS Managed Policy AmazonDynamoDBFullAccess and attach to Lambda functions.
C)	
Use AmazonDynamoDBFullAccess policy template in the SAM template.
D)	
Use DynamoDBCrudPolicy policy template in the SAM template.

As the team lead for an application, you have been tasked with utilizing Jenkins as the Build provider in AWS CodePipeline. What is considered the best practice in this scenario?
A)	
Install Jenkins on a Lambda function so that it is serverless and cost-effective.
B)	
Install Jenkins on an EC2 instance and make sure the instance role has the "codepipeline" permission.
C)	
Add Access Keys in EC2 for the Jenkins server to access CodePipeline.
D)	
Install Jenkins on an Amazon EC2 instance. Make sure the instance profile grants Jenkins, only the AWS permissions required to perform tasks for your project.

You intend to utilize the AWS CodeDeploy tool for deploying your application. What is used to define the deployment process to the underlying instances?
A)	
appspec.yml
B)	
appConfig.json
C)	
DeploymentGroup
D)	
appConfig.YAML

A corporation is utilizing a DynamoDB table in their application. It is now necessary to guarantee that all modifications made to the elements in the table are documented and saved in a MySQL database. Which of the following would ideally be one of the measures for implementing the plan?
A)	
Enable DynamoDB triggers
B)	
Enable DynamoDB Accelerator
C)	
Enable DynamoDB global tables
D)	
Enable DynamoDB streams

You have recently set up a Lambda function that is located behind the API gateway service. Upon attempting to call the Lambda function through the API gateway service from JavaScript in your HTML page, you encounter the following issue: "No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'null' is therefore not allowed access." What steps can be taken to fix this problem?
A)	
Change the IAM policy for the API gateway to enable anonymous access.
B)	
Enable CORS for the lambda function.
C)	
Enable CORS for the methods in the API gateway.
D)	
Change the IAM policy for the Lambda function to enable anonymous access.

As a newly appointed Database administrator at a start-up, your initial responsibility involves adjusting a DynamoDB table to automatically expire data after a specific timeframe. Upon reviewing the documentation, you discovered that DynamoDB offers a Time to Live (TTL) feature that enables this functionality. What steps should be taken to implement this feature?
A)	
It is by default enabled and will automatically pick a key attribute with timestamp value.
B)	
Enable TTL and use the dataExpiry keyword as a key attribute to store the expiry timestamp.
C)	
Enable TTL and use any name of your choice as a key attribute to store the expiry timestamp.
D)	
Enable TTL and use the keyword expiryTTL as a key attribute to store the expiry timestamp.

Your client's web application is hosted on-prem and has recently shifted its database service to DynamoDB. The application is experiencing over 1000 read requests per second and around 200 write requests per second. Your client has requested the implementation of a caching mechanism to enhance the speed of read requests and reduce costs on RCUs. What modifications are necessary to achieve this?
A)	
None of the below.
B)	
Create an ElastiCache redis cluster and configure your application to write the most frequently accessed data to redis for faster read queries.
C)	
Place DynamoDB accelerator in front of the database to cache frequently accessed data.
D)	
Place an ElastiCache Memcached cluster in front of DynamoDB to cache the requests.

Your stock market trading application sends real-time data to AWS Kinesis, which is then processed and sorted by a Lambda function before being saved to a DynamoDB table. Customers access this table through a dashboard. Upon market opening, customers report missing data. Which Lambda CloudWatch metric should you prioritize to address this issue?
A)	
IteratorAge
B)	
Throttles
C)	
Dwell time
D)	
ConcurrentExecutions

The financial institute has chosen to store its important documents in an Amazon S3 bucket with versioning enabled. Despite applying a retention period using object lock, there are instances where certain objects are being overwritten. What could be a potential cause for this issue?
A)	
Object locks need to apply to all versions of the objects in a versioned bucket.
B)	
Object should have both retention period and legal hold to prevent from overwriting.
C)	
Object locks apply to a specific version of an object in a versioned bucket.
D)	
Object lock should be applied at bucket level to prevent all objects in a bucket from overwriting.

What are the potential issues with the requests that are causing some of the request results to return an HTTP 4xx status code while developing an application that works with a DynamoDB table? Choose two answers from the options listed below.
A)	
There are network issues.
B)	
There are missing required parameters with some of the requests.
C)	
You are exceeding the table's provisioned throughput.
D)	
The DynamoDB service is unavailable.
---------------------------------------



{
"timestamp": "2025-03-15T09:54:47.068+00:00",
"status": 500,
"error": "Internal Server Error",
"trace": "org.springframework.orm.jpa.JpaSystemException: could not execute statement [Field 'user_id' doesn't have a default value] [insert into users (email,name) values (?,?)]\r\n\tat org.springframework.orm.jpa.vendor.HibernateJpaDialect.convertHibernateAccessException(HibernateJpaDialect.java:341)\r\n\tat org.springframework.orm.jpa.vendor.HibernateJpaDialect.translateExceptionIfPossible(HibernateJpaDialect.java:241)\r\n\tat org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.translateExceptionIfPossible(AbstractEntityManagerFactoryBean.java:560)\r\n\tat org.springframework.dao.support.ChainedPersistenceExceptionTranslator.translateExceptionIfPossible(ChainedPersistenceExceptionTranslator.java:61)\r\n\tat org.springframework.dao.support.DataAccessUtils.translateIfNecessary(DataAccessUtils.java:343)\r\n\tat org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:160)\r\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184)\r\n\tat org.springframework.data.jpa.repository.support.CrudMethodMetadataPostProcessor$CrudMethodMetadataPopulatingMethodInterceptor.invoke(CrudMethodMetadataPostProcessor.java:165)\r\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184)\r\n\tat org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:223)\r\n\tat jdk.proxy4/jdk.proxy4.$Proxy119.save(Unknown Source)\r\n\tat com.example.springbootmysql.service.UserService.createUser(UserService.java:25)\r\n\tat com.example.springbootmysql.controller.UserController.createUser(UserController.java:32)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:257)\r\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:190)\r\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:118)\r\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:986)\r\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:891)\r\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87)\r\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1088)\r\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:978)\r\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1014)\r\n\tat org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:914)\r\n\tat jakarta.servlet.http.HttpServlet.service(HttpServlet.java:590)\r\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:885)\r\n\tat jakarta.servlet.http.HttpServlet.service(HttpServlet.java:658)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:195)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:140)\r\n\tat org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:51)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:164)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:140)\r\n\tat org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100)\r\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:164)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:140)\r\n\tat org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93)\r\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:164)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:140)\r\n\tat org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201)\r\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:116)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:164)\r\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:140)\r\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:167)\r\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:90)\r\n\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:483)\r\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:115)\r\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:93)\r\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:74)\r\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:344)\r\n\tat org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:397)\r\n\tat org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:63)\r\n\tat org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:905)\r\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1743)\r\n\tat org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:52)\r\n\tat org.apache.tomcat.util.threads.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1190)\r\n\tat org.apache.tomcat.util.threads.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:659)\r\n\tat org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:63)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.hibernate.exception.GenericJDBCException: could not execute statement [Field 'user_id' doesn't have a default value] [insert into users (email,name) values (?,?)]\r\n\tat org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:63)\r\n\tat org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)\r\n\tat org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.executeUpdate(ResultSetReturnImpl.java:197)\r\n\tat org.hibernate.id.insert.GetGeneratedKeysDelegate.performMutation(GetGeneratedKeysDelegate.java:116)\r\n\tat org.hibernate.engine.jdbc.mutation.internal.MutationExecutorSingleNonBatched.performNonBatchedOperations(MutationExecutorSingleNonBatched.java:47)\r\n\tat org.hibernate.engine.jdbc.mutation.internal.AbstractMutationExecutor.execute(AbstractMutationExecutor.java:55)\r\n\tat org.hibernate.persister.entity.mutation.InsertCoordinatorStandard.doStaticInserts(InsertCoordinatorStandard.java:194)\r\n\tat org.hibernate.persister.entity.mutation.InsertCoordinatorStandard.coordinateInsert(InsertCoordinatorStandard.java:132)\r\n\tat org.hibernate.persister.entity.mutation.InsertCoordinatorStandard.insert(InsertCoordinatorStandard.java:95)\r\n\tat org.hibernate.action.internal.EntityIdentityInsertAction.execute(EntityIdentityInsertAction.java:85)\r\n\tat org.hibernate.engine.spi.ActionQueue.execute(ActionQueue.java:682)\r\n\tat org.hibernate.engine.spi.ActionQueue.addResolvedEntityInsertAction(ActionQueue.java:293)\r\n\tat org.hibernate.engine.spi.ActionQueue.addInsertAction(ActionQueue.java:274)\r\n\tat org.hibernate.engine.spi.ActionQueue.addAction(ActionQueue.java:324)\r\n\tat org.hibernate.event.internal.AbstractSaveEventListener.addInsertAction(AbstractSaveEventListener.java:393)\r\n\tat org.hibernate.event.internal.AbstractSaveEventListener.performSaveOrReplicate(AbstractSaveEventListener.java:307)\r\n\tat org.hibernate.event.internal.AbstractSaveEventListener.performSave(AbstractSaveEventListener.java:223)\r\n\tat org.hibernate.event.internal.AbstractSaveEventListener.saveWithGeneratedId(AbstractSaveEventListener.java:136)\r\n\tat org.hibernate.event.internal.DefaultPersistEventListener.entityIsTransient(DefaultPersistEventListener.java:177)\r\n\tat org.hibernate.event.internal.DefaultPersistEventListener.persist(DefaultPersistEventListener.java:95)\r\n\tat org.hibernate.event.internal.DefaultPersistEventListener.onPersist(DefaultPersistEventListener.java:79)\r\n\tat org.hibernate.event.internal.DefaultPersistEventListener.onPersist(DefaultPersistEventListener.java:55)\r\n\tat org.hibernate.event.service.internal.EventListenerGroupImpl.fireEventOnEachListener(EventListenerGroupImpl.java:127)\r\n\tat org.hibernate.internal.SessionImpl.firePersist(SessionImpl.java:761)\r\n\tat org.hibernate.internal.SessionImpl.persist(SessionImpl.java:745)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat org.springframework.orm.jpa.ExtendedEntityManagerCreator$ExtendedEntityManagerInvocationHandler.invoke(ExtendedEntityManagerCreator.java:364)\r\n\tat jdk.proxy4/jdk.proxy4.$Proxy115.persist(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat org.springframework.orm.jpa.SharedEntityManagerCreator$SharedEntityManagerInvocationHandler.invoke(SharedEntityManagerCreator.java:320)\r\n\tat jdk.proxy4/jdk.proxy4.$Proxy115.persist(Unknown Source)\r\n\tat org.springframework.data.jpa.repository.support.SimpleJpaRepository.save(SimpleJpaRepository.java:636)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:359)\r\n\tat org.springframework.data.repository.core.support.RepositoryMethodInvoker$RepositoryFragmentMethodInvoker.lambda$new$0(RepositoryMethodInvoker.java:277)\r\n\tat org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:170)\r\n\tat org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:158)\r\n\tat org.springframework.data.repository.core.support.RepositoryComposition$RepositoryFragments.invoke(RepositoryComposition.java:515)\r\n\tat org.springframework.data.repository.core.support.RepositoryComposition.invoke(RepositoryComposition.java:284)\r\n\tat org.springframework.data.repository.core.support.RepositoryFactorySupport$ImplementationMethodExecutionInterceptor.invoke(RepositoryFactorySupport.java:752)\r\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184)\r\n\tat org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:174)\r\n\tat org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:149)\r\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184)\r\n\tat org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:69)\r\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184)\r\n\tat org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:380)\r\n\tat org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:119)\r\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:184)\r\n\tat org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:138)\r\n\t... 55 more\r\nCaused by: java.sql.SQLException: Field 'user_id' doesn't have a default value\r\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:121)\r\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)\r\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)\r\n\tat com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61)\r\n\tat com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeUpdate(HikariProxyPreparedStatement.java)\r\n\tat org.hibernate.engine.jdbc.internal.ResultSetReturnImpl.executeUpdate(ResultSetReturnImpl.java:194)\r\n\t... 105 more\r\n",
"message": "could not execute statement [Field 'user_id' doesn't have a default value] [insert into users (email,name) values (?,?)]",
"path": "/api/users"
}
